{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e5026-004f-4d1f-9a83-a6f4f9a72f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get COVID-19 HGI fine-mapping files\n",
    "wget https://storage.googleapis.com/covid19-hg-in-silico-followup/V4/finemapping/COVID19_HGI_20201020_ABF.tar.gz\n",
    "unzip COVID19_HGI_20201020_ABF.tar.gz\n",
    "\n",
    "# Get ABC predictions\n",
    "wget https://mitra.stanford.edu/engreitz/oak/public/Nasser2021/AllPredictions.AvgHiC.ABC0.015.minus150.ForABCPaperV3.txt.gz\n",
    "\n",
    "## Ensembl hg38 annotation file\n",
    "## https://useast.ensembl.org/biomart/martview/75c3078f9cf760aa9f485728508be51b\n",
    "## Download the columns * Chromosome/scaffold name * Gene start (bp) * Gene end (bp) * Gene stable ID * Gene name * Strand * Gene type\n",
    "## Save as hg38_ensembl.txt and hg38_ensembl.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e0472-03d1-4ce3-bbf3-d49fb4597dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ABC result from hg19 to hg38\n",
    "!awk 'BEGIN {{OFS=\"\\t\"; print \"chr\", \"start\", \"end\", \"idx\"}} NR > 1 {{print $1, $2, $3, NR-1}}' AllPredictions.AvgHiC.ABC0.015.minus150.ForABCPaperV3.txt > abc_result_hg19.bed\n",
    "!crossmap bed hg19ToHg38.over.chain.gz abc_result_hg19.bed abc_result_hg38.bed\n",
    "!echo -e \"chr\\tstart\\tend\" > abc_result_hg38wH.bed && cat abc_result_hg38.bed >> abc_result_hg38wH.bed\n",
    "!rm abc_result_hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9dc14-ed4a-47bd-a7a0-4731bb439939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hg19 to hg38 Harmonization\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    abc38 = pd.read_csv('abc_result_hg38wH.bed',\n",
    "                        sep='\\t', header=0)\n",
    "    abc19 = pd.read_csv('abc_result_hg19.bed',\n",
    "                        sep='\\t', header=0)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found {e.filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Calculate counts\n",
    "abc19_counts = abc19['idx'].value_counts()\n",
    "abc38_counts = abc38['idx'].value_counts()\n",
    "\n",
    "df_abc19_idxes = abc19_counts.reset_index()\n",
    "df_abc19_idxes.columns = ['idx', 'abc19_counts']\n",
    "\n",
    "df_abc38_idxes = abc38_counts.reset_index()\n",
    "df_abc38_idxes.columns = ['idx', 'abc38_counts']\n",
    "\n",
    "# Merge counts on idx\n",
    "merged_counts_idxes = pd.merge(\n",
    "    df_abc19_idxes, df_abc38_idxes, on='idx', how='outer').fillna(0)\n",
    "\n",
    "# Convert counts to integers\n",
    "merged_counts_idxes['abc19_counts'] = merged_counts_idxes['abc19_counts'].astype(\n",
    "    int)\n",
    "merged_counts_idxes['abc38_counts'] = merged_counts_idxes['abc38_counts'].astype(\n",
    "    int)\n",
    "\n",
    "# Output increased idx\n",
    "increased_idxes = merged_counts_idxes[merged_counts_idxes['abc38_counts']\n",
    "                                      > merged_counts_idxes['abc19_counts']]\n",
    "\n",
    "# Handle increases\n",
    "for index, row in increased_idxes.iterrows():\n",
    "    idx = row['idx']\n",
    "    count_to_keep = row['abc19_counts']\n",
    "    rows_to_keep = abc38[abc38['idx'] == idx].head(count_to_keep)\n",
    "    abc38 = pd.concat([abc38[abc38['idx'] != idx],\n",
    "                      rows_to_keep], ignore_index=True)\n",
    "\n",
    "# Handle Missing idx\n",
    "ids_abc19 = set(abc19['idx'])\n",
    "ids_abc38 = set(abc38['idx'])\n",
    "\n",
    "missing_in_abc38 = ids_abc19.difference(ids_abc38)\n",
    "missing_abc19_data = abc19[abc19['idx'].isin(missing_in_abc38)]\n",
    "\n",
    "# Create rows of missing IDs for 1:1 merging\n",
    "missing_rows = [{\n",
    "    'chr': row['chr'],\n",
    "    'start': pd.NA,\n",
    "    'end': pd.NA,\n",
    "    'idx': row['idx'],\n",
    "} for _, row in missing_abc19_data.iterrows()]\n",
    "\n",
    "missing_df = pd.DataFrame(missing_rows)\n",
    "abc38_complete = pd.concat([abc38, missing_df], ignore_index=True)\n",
    "\n",
    "abc38_complete.to_csv('final_abc38_complete.bed', sep='\\t', index=False)\n",
    "print('hg19 to hg38 Harmonization successfully done and save as: final_abc38_complete.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1d020-185c-4746-a4b4-89a275ba0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ABC Allpredictions\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original data\n",
    "original_df = pd.read_csv(\n",
    "    'AllPredictions.AvgHiC.ABC0.015.minus150.ForABCPaperV3.txt', sep='\\t')\n",
    "\n",
    "# Load the converted coordinates data\n",
    "converted_df = pd.read_csv('final_abc38_complete.bed', sep='\\t')\n",
    "\n",
    "# Ensure that the 'idx' column in the converted DataFrame is of the right data type\n",
    "converted_df['idx'] = converted_df['idx'].astype(int)\n",
    "\n",
    "# Add an 'idx' column to the original DataFrame that matches the row index + 1 (to match your 1-based idx)\n",
    "original_df['idx'] = range(1, len(original_df) + 1)\n",
    "\n",
    "# Merge the original DataFrame with the converted coordinates on 'idx'\n",
    "merged_df = pd.merge(original_df, converted_df[[\n",
    "                     'chr', 'start', 'end', 'idx']], on='idx', suffixes=('', '_converted'))\n",
    "\n",
    "\n",
    "# Update the original 'chr', 'start', and 'end' columns with the converted values\n",
    "merged_df['chr'] = merged_df['chr_converted']\n",
    "merged_df['start'] = merged_df['start_converted']\n",
    "merged_df['end'] = merged_df['end_converted']\n",
    "\n",
    "# Drop the converted columns as they are no longer needed\n",
    "merged_df.drop(['chr_converted', 'start_converted',\n",
    "               'end_converted'], axis=1, inplace=True)\n",
    "\n",
    "merged_df.drop(['idx'], axis=1, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "merged_df.to_csv('ABC_AllPredictions_hg38.txt', sep='\\t', index=False)\n",
    "\n",
    "print('ABC.Predicitons coordinats successfully updated and save as: ABC_AllPredictions_hg38.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96518abd-963a-4510-b072-d1a1ecb9abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure pyranges is installed \n",
    "import os\n",
    "os.system(\"pip install pyranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a383882-6721-4566-9a98-ee270b66e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyranges as pr\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Prepare abc_hg38 data\n",
    "abc_hg38_df = pd.read_csv('ABC_AllPredictions_hg38.txt', sep='\\t')\n",
    "abc_hg38_df.dropna(subset=['start', 'end'], inplace=True)\n",
    "abc_hg38_df = abc_hg38_df.astype({'start': int, 'end': int})\n",
    "abc_hg38_df.rename(columns={\"chr\": \"Chromosome\",\n",
    "                   \"start\": \"Start\", \"end\": \"End\"}, inplace=True)\n",
    "abc_hg38_df['Chromosome'] = abc_hg38_df['Chromosome'].astype(str)\n",
    "abc_hg38_df.dropna(subset=['Start', 'End', 'Chromosome'], inplace=True)\n",
    "abc_hg38_ranges = pr.PyRanges(abc_hg38_df)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare fine-mapping files\n",
    "directory_path = './COVID19_HGI_20201020_ABF/results/'\n",
    "fine_mapping_files = [\n",
    "    'COVID19_HGI_A2_ALL_leave_23andme_20201020.ABF.snp.bgz',\n",
    "    'COVID19_HGI_B1_ALL_20201020.ABF.snp.bgz',\n",
    "    'COVID19_HGI_B2_ALL_leave_23andme_20201020.ABF.snp.bgz',\n",
    "    'COVID19_HGI_C1_ALL_leave_23andme_20201020.ABF.snp.bgz',\n",
    "    'COVID19_HGI_C2_ALL_leave_23andme_20201020.ABF.snp.bgz',\n",
    "    'COVID19_HGI_D1_ALL_20201020.ABF.snp.bgz',\n",
    "]\n",
    "\n",
    "# Prepare ensembl annotation file\n",
    "annotation_data = pd.read_csv('hg38_ensembl.txt', sep='\\t')\n",
    "annotation_data['Chromosome'] = 'chr' + annotation_data['Chromosome/scaffold name'].astype(str)\n",
    "annotation_data['Start'] = annotation_data['Gene start (bp)']\n",
    "annotation_data['End'] = annotation_data['Gene end (bp)']\n",
    "annotation_data = annotation_data[['Chromosome', 'Start', 'End', 'Gene stable ID']]\n",
    "\n",
    "\n",
    "annotation_ranges = pr.PyRanges(annotation_data)\n",
    "\n",
    "\n",
    "# Process each file\n",
    "for file_name in fine_mapping_files:\n",
    "    file_path = directory_path + file_name\n",
    "    try:\n",
    "        df_variants = pd.read_csv(\n",
    "            file_path, compression='gzip', sep='\\t', header=0)\n",
    "        if 'cs_99' not in df_variants.columns or 'position' not in df_variants.columns or 'chromosome' not in df_variants.columns:\n",
    "            print(f\"Missing required columns in {file_name}.\")\n",
    "            continue\n",
    "        # Filter variants by cs_99 ==1\n",
    "        filtered_df = df_variants[df_variants['cs_99'] == 1]\n",
    "        filtered_df = filtered_df.dropna(subset=['position'])\n",
    "        filtered_df.loc[:, 'position'] = filtered_df['position'].astype(int)\n",
    "        filtered_df.loc[:,\n",
    "                        'Chromosome'] = filtered_df['chromosome'].astype(str)\n",
    "        filtered_df.loc[:, 'Start'] = filtered_df['position']\n",
    "        filtered_df.loc[:, 'End'] = filtered_df['position']\n",
    "        filtered_df = filtered_df.dropna(subset=['Chromosome', 'Start', 'End'])\n",
    "\n",
    "        gr_variants = pr.PyRanges(filtered_df[['Chromosome', 'Start', 'End']])\n",
    "        \n",
    "        # Map SNP position to promoter region\n",
    "        overlaps = abc_hg38_ranges.join(gr_variants, suffix='_variant')\n",
    "        print(overlaps)\n",
    "        annotated_overlap=overlaps.join(annotation_ranges, suffix='_annotation')\n",
    "        print(annotated_overlap)\n",
    "        result_df = annotated_overlap.df\n",
    "\n",
    "        # Sort results by ABC.Score in descending order\n",
    "        if 'ABC.Score' in result_df.columns:\n",
    "            result_df = result_df.sort_values(by='ABC.Score', ascending=False)\n",
    "        \n",
    "        output_filename = 'ABC_results_' + file_name.replace('.ABF.snp.bgz', '.tsv')\n",
    "        result_df.to_csv(directory_path + output_filename,\n",
    "                         sep='\\t', index=False)\n",
    "        print(f\"Results saved successfully for {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39db95-0521-4273-b4d2-746e4c82f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output ABC results and file for PPI analysis\n",
    "import pandas as pd\n",
    "\n",
    "directory_path = './COVID19_HGI_20201020_ABF/results/'\n",
    "fine_mapping_files = [\n",
    "    'ABC_results_COVID19_HGI_A2_ALL_leave_23andme_20201020.tsv',\n",
    "    'ABC_results_COVID19_HGI_B1_ALL_20201020.tsv',\n",
    "    'ABC_results_COVID19_HGI_B2_ALL_leave_23andme_20201020.tsv',\n",
    "    'ABC_results_COVID19_HGI_C1_ALL_leave_23andme_20201020.tsv',\n",
    "    'ABC_results_COVID19_HGI_C2_ALL_leave_23andme_20201020.tsv',\n",
    "    'ABC_results_COVID19_HGI_D1_ALL_20201020.tsv',\n",
    "]\n",
    "\n",
    "abc_all = pd.DataFrame()\n",
    "\n",
    "for file_name in fine_mapping_files:\n",
    "    file_path = directory_path + file_name\n",
    "    try:\n",
    "        df_variants = pd.read_csv(file_path, sep='\\t')\n",
    "        if not df_variants.empty:\n",
    "            abc_all = pd.concat([abc_all, df_variants], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Data frame is empty after loading: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file_name}: {e}\")\n",
    "\n",
    "abc_all = abc_all.drop_duplicates(subset=['Gene stable ID', 'TargetGene'])\n",
    "try:\n",
    "    abc_all['Chromosome'] = abc_all['Chromosome'].str.replace('chr', '').astype(int)\n",
    "    abc_all.sort_values('Chromosome', inplace=True)\n",
    "    abc_all['Chromosome'] = 'chr' + abc_all['Chromosome'].astype(str)\n",
    "    abc_all_path = 'abc_all.tsv'\n",
    "    \n",
    "    # For PPI analysis used\n",
    "    abc_ensg = abc_all[['Gene stable ID', 'ABC.Score']]\n",
    "    abc_ensg = abc_ensg.sort_values(by='ABC.Score', ascending=False)\n",
    "    abc_ensg_path = 'abc_ensg.tsv'\n",
    "    \n",
    "except ValueError as e:\n",
    "    print('If your result includes chromosome X and Y, please change this code accordingly')\n",
    "    \n",
    "abc_all.to_csv(abc_all_path, sep='\\t', index=False)\n",
    "abc_ensg.to_csv(abc_ensg_path, sep='\\t', index=False) \n",
    "print(f\"file processed and save to {abc_ensg_path} and {abc_all_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e92f9-b2bc-4fc4-81e3-ba4fe9fc0978",
   "metadata": {},
   "source": [
    "#COGS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b6bf2-6af8-4234-b681-f80ac12c57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software installation\n",
    "import os\n",
    "\n",
    "## rCOGS\n",
    "os.system('git clone https://github.com/ollyburren/rCOGS.git')\n",
    "\n",
    "## crossmap\n",
    "os.system('pip3 install CrossMap')\n",
    "\n",
    "## pyrangers\n",
    "os.system('pip3 install pyranges')\n",
    "\n",
    "##Biopython\n",
    "os.system('pip3 install biopython')\n",
    "\n",
    "## ensembl-VEP (https://useast.ensembl.org/info/docs/tools/vep/script/vep_download.html)\n",
    "os.system('git clone https://github.com/Ensembl/ensembl-vep.git')\n",
    "os.chdir('ensembl-vep')\n",
    "os.system('perl INSTALL.pl')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e8369-4e36-4dd7-b240-a69488781de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Region file Hapmap II hg38\n",
    "!wget https://storage.googleapis.com/broad-alkesgroup-public/Eagle/downloads/tables/genetic_map_hg38_withX.txt.gz\n",
    "\n",
    "## Finemapping files\n",
    "!wget https://storage.googleapis.com/covid19-hg-in-silico-followup/V4/finemapping/COVID19_HGI_20201020_ABF.tar.gz\n",
    "!gzip -d COVID19_HGI_20201020_ABF.tar.gz\n",
    "!tar -xf COVID19_HGI_20201020_ABF.tar\n",
    "!rm COVID19_HGI_20201020_ABF.tar\n",
    "\n",
    "## pcHi-C Peak Matrix \n",
    "!wget -O PCHiC_peak_matrix_cutoff5.txt.gz https://osf.io/download/63hh4/\n",
    "!gzip -d PCHiC_peak_matrix_cutoff5.txt.gz\n",
    "    \n",
    "## Chain and fa file for conversion\n",
    "!wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\n",
    "!wget https://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/hg19ToHg38.over.chain.gz\n",
    "!gzip -d hg38.fa.gz\n",
    "!gzip -d hg19ToHg38.over.chain.gz\n",
    "\n",
    "## Ensembl hg38 annotation file\n",
    "## https://useast.ensembl.org/biomart/martview/75c3078f9cf760aa9f485728508be51b\n",
    "## Download the columns * Chromosome/scaffold name * Gene start (bp) * Gene end (bp) * Gene stable ID * Gene name * Strand * Gene type\n",
    "## Save as hg38_ensembl.txt and hg38_ensembl.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f771c-c636-47cc-bf5c-91ab11013cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Region file\n",
    "!bash processGeneticMap2Region.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f95444-b1a6-4f31-82eb-a2428559f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process MAF and Trait Association file\n",
    "!bash processMAFtraitAsso.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d3827-b486-4427-80c1-fc9765dd2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process pcHiC file\n",
    "## Convert from hg19 to hg38\n",
    "!sed 's/,/\\t/g' PCHiC_peak_matrix_cutoff5.txt > PCHiC_peak_matrix_cutoff5.tsv\n",
    "!awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4}' PCHiC_peak_matrix_cutoff5.tsv > bait_coordinates.bed \n",
    "!crossmap bed hg19ToHg38.over.chain bait_coordinates.bed bait_coordinates_hg38.bed\n",
    "!(echo \"baitChr\\tbaitStart\\tbaitEnd\\tbaitID\" && cat bait_coordinates_hg38.bed) > bait_hg38.bed\n",
    "\n",
    "!awk '{print $6\"\\t\"$7\"\\t\"$8\"\\t\"$9\"\\t\"$4}' PCHiC_peak_matrix_cutoff5.tsv > oe_coordinates.bed\n",
    "!crossmap bed hg19ToHg38.over.chain oe_coordinates.bed oe_coordinates_hg38.bed\n",
    "!(echo \"oeChr\\toeStart\\toeEnd\\toeID\\tbaitID\" && cat oe_coordinates_hg38.bed) > oe_hg38.bed\n",
    "\n",
    "!rm oe_coordinates_hg38.bed bait_coordinates_hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d2805-faf1-4ae2-ace4-05f24a9608bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## baitHarmonization\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    bait38 = pd.read_csv('bait_hg38.bed', sep='\\t', header=0, dtype={\n",
    "                         'baitID': str}, low_memory=False)\n",
    "    bait37 = pd.read_csv('bait_coordinates.bed', sep='\\t',\n",
    "                         header=0, dtype={'baitID': str}, low_memory=False)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Check if two dataset have same ID\n",
    "# print(set(bait37[\"baitID\"]) == set(bait38[\"baitID\"]))\n",
    "\n",
    "# Calculate counts of each baitID in both datasets\n",
    "count37 = bait37['baitID'].value_counts()\n",
    "count38 = bait38['baitID'].value_counts()\n",
    "\n",
    "# Convert to DataFrame for easier comparison\n",
    "df_count37 = count37.reset_index()\n",
    "df_count37.columns = ['baitID', 'count_hg19']\n",
    "\n",
    "df_count38 = count38.reset_index()\n",
    "df_count38.columns = ['baitID', 'count_hg38']\n",
    "\n",
    "# Merge counts on baitID\n",
    "merged_counts = pd.merge(df_count37, df_count38,\n",
    "                         on='baitID', how='outer').fillna(0)\n",
    "\n",
    "# Find baitIDs with increased counts in hg38\n",
    "increased_baitIDs = merged_counts[merged_counts['count_hg38']\n",
    "                                  > merged_counts['count_hg19']]\n",
    "\n",
    "# Output increased baitIDs\n",
    "print(\"Increased baitIDs in hg38:\")\n",
    "print(increased_baitIDs[['baitID', 'count_hg19', 'count_hg38']])\n",
    "\n",
    "# Handle increases\n",
    "for index, row in increased_baitIDs.iterrows():\n",
    "    bait_id = row['baitID']\n",
    "    count_to_keep = int(row['count_hg19'])\n",
    "    rows_to_keep = bait38[bait38['baitID'] == bait_id].head(count_to_keep)\n",
    "\n",
    "    # Filtering and keeping only the required count of duplicates\n",
    "    bait38 = pd.concat([bait38[bait38['baitID'] != bait_id], rows_to_keep])\n",
    "\n",
    "\n",
    "# Save the final hg38 dataset\n",
    "bait38.to_csv('final_bait_coordinates_hg38.bed', sep='\\t', index=False)\n",
    "print('baitHarmonization done, and save as file: final_bait_coordinates_hg38.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02288a7f-3df6-4eb9-a045-4aa5f87f9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "## oeHarmonization (To Do: Combine Harmonization scripts)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    oe37 = pd.read_csv('oe_coordinates.bed', sep='\\t',\n",
    "                       header=0, dtype={'oeID': str}, low_memory=False)\n",
    "    oe38 = pd.read_csv('oe_hg38.bed', sep='\\t', header=0,\n",
    "                       dtype={'oeID': str}, low_memory=False)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Calculate counts of each oeID in both datasets\n",
    "count37_oe = oe37['oeID'].value_counts()\n",
    "count38_oe = oe38['oeID'].value_counts()\n",
    "\n",
    "# Convert to DataFrame for easier comparison\n",
    "df_count37_oe = count37_oe.reset_index()\n",
    "df_count37_oe.columns = ['oeID', 'count_hg19']\n",
    "\n",
    "df_count38_oe = count38_oe.reset_index()\n",
    "df_count38_oe.columns = ['oeID', 'count_hg38']\n",
    "\n",
    "# Merge counts on oeID\n",
    "merged_counts_oe = pd.merge(\n",
    "    df_count37_oe, df_count38_oe, on='oeID', how='outer').fillna(0)\n",
    "\n",
    "# Convert counts to integers after filling NA\n",
    "merged_counts_oe['count_hg19'] = merged_counts_oe['count_hg19'].astype(\n",
    "    int)\n",
    "merged_counts_oe['count_hg38'] = merged_counts_oe['count_hg38'].astype(\n",
    "    int)\n",
    "\n",
    "# Output increased oeIDs\n",
    "increased_oeIDs = merged_counts_oe[merged_counts_oe['count_hg38']\n",
    "                                   > merged_counts_oe['count_hg19']]\n",
    "print(\"Increased oeIDs in hg38 for oe datasets:\")\n",
    "print(increased_oeIDs[['oeID', 'count_hg19', 'count_hg38']])\n",
    "\n",
    "\n",
    "# Handle increases\n",
    "for index, row in increased_oeIDs.iterrows():\n",
    "    oe_id = row['oeID']\n",
    "    count_to_keep = row['count_hg19']\n",
    "    rows_to_keep = oe38[oe38['oeID'] == oe_id].head(count_to_keep)\n",
    "    oe38 = pd.concat([oe38[oe38['oeID'] != oe_id],\n",
    "                     rows_to_keep], ignore_index=True)\n",
    "\n",
    "print(oe38)\n",
    "\n",
    "# Handling Missing oeID\n",
    "ids_oe37 = set(oe37['oeID'])\n",
    "ids_oe38 = set(oe38['oeID'])\n",
    "\n",
    "missing_in_oe38 = ids_oe37.difference(ids_oe38)\n",
    "missing_oe37_data = oe37[oe37['oeID'].isin(missing_in_oe38)]\n",
    "\n",
    "print(missing_in_oe38)\n",
    "print(missing_oe37_data)\n",
    "# Create rows for missing IDs with NaN for chr, start, and end\n",
    "missing_rows = [{\n",
    "    'oeChr': row['oeChr'],\n",
    "    'oeStart': pd.NA,\n",
    "    'oeEnd': pd.NA,\n",
    "    'oeID': row['oeID'],\n",
    "    'baitID': row['baitID']\n",
    "} for _, row in missing_oe37_data.iterrows()]\n",
    "\n",
    "# Append missing rows to oe38 using pd.concat\n",
    "missing_df = pd.DataFrame(missing_rows)\n",
    "oe38 = pd.concat([oe38, missing_df], ignore_index=True)\n",
    "\n",
    "oe38['oeStart'] = oe38['oeStart'].astype('Int64')\n",
    "oe38['oeEnd'] = oe38['oeEnd'].astype('Int64')\n",
    "\n",
    "print(oe38)\n",
    "# Save the filtered hg38 dataset\n",
    "oe38.to_csv('final_oe_coordinates_hg38.bed', sep='\\t', index=False)\n",
    "print('oeHarmonization done and save as file: final_oe_coordinates_hg38.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c6533-446c-47c9-8ed8-471eef2c2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data specifying low_memory=False to handle mixed types more robustly\n",
    "data = pd.read_csv('PCHiC_peak_matrix_cutoff5.tsv', sep='\\t',\n",
    "                   dtype={'baitID': str, 'oeID': str}, low_memory=False)\n",
    "bait_hg38 = pd.read_csv('final_bait_coordinates_hg38.bed', sep='\\t', header=0, names=[\n",
    "                        'baitChr', 'baitStart', 'baitEnd', 'baitID'], dtype={'baitID': str}, low_memory=False)\n",
    "oe_hg38 = pd.read_csv('final_oe_coordinates_hg38.bed', sep='\\t', header=0, names=[\n",
    "                      'oeChr', 'oeStart', 'oeEnd', 'oeID', 'baitID'], dtype={'oeID': str, 'baitID': str}, low_memory=False)\n",
    "\n",
    "\n",
    "# Align bait by baitID\n",
    "bait_hg38['baitID'] = pd.to_numeric(bait_hg38['baitID'], errors='coerce')\n",
    "data['baitID'] = pd.to_numeric(\n",
    "    data['baitID'], errors='coerce', downcast='integer')\n",
    "\n",
    "# Sort dataframes by IDs numerically\n",
    "bait_hg38.sort_values(by='baitID', inplace=True)\n",
    "data.sort_values(by='baitID', inplace=True)\n",
    "\n",
    "# Reset indices of both dataframes after sorting\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "bait_hg38.reset_index(drop=True, inplace=True)\n",
    "\n",
    "bait_mismatch = (data['baitID'] != bait_hg38['baitID'])\n",
    "if bait_mismatch.any():\n",
    "    print(\"baitID alignment failed. Check data consistency.\")\n",
    "else:\n",
    "    print(\"baitID alignment successful. Proceeding with updates.\")\n",
    "    data['baitChr'] = bait_hg38['baitChr'].values\n",
    "    data['baitStart'] = bait_hg38['baitStart'].values\n",
    "    data['baitEnd'] = bait_hg38['baitEnd'].values\n",
    "\n",
    "print(data)\n",
    "print(bait_hg38)\n",
    "\n",
    "# Align bait by oeID\n",
    "oe_hg38['oeID'] = pd.to_numeric(oe_hg38['oeID'], errors='coerce')\n",
    "data['oeID'] = pd.to_numeric(\n",
    "    data['oeID'], errors='coerce', downcast='integer')\n",
    "\n",
    "oe_hg38.sort_values(by='oeID', inplace=True)\n",
    "data.sort_values(by='oeID', inplace=True)\n",
    "\n",
    "oe_hg38.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(oe_hg38)\n",
    "print(data)\n",
    "oe_mismatch = (data['oeID'] != oe_hg38['oeID'])\n",
    "if oe_mismatch.any():\n",
    "    print(\"oeID alignment failed. Check data consistency.\")\n",
    "else:\n",
    "    print(\"oeID alignment successful. Proceeding with updates.\")\n",
    "    data['oeChr'] = oe_hg38['oeChr'].values\n",
    "    data['oeStart'] = oe_hg38['oeStart'].values\n",
    "    data['oeEnd'] = oe_hg38['oeEnd'].values\n",
    "\n",
    "print(data)\n",
    "print(oe_hg38)\n",
    "\n",
    "# Save updated data\n",
    "data.to_csv('PCHiC_peak_matrix_hg38_final.tsv', sep='\\t', index=False)\n",
    "\n",
    "print('Bait and oe coordinates successfully updated and saved as: PCHiC_peak_matrix_hg38_final.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52513878-e851-45c7-83fd-7ffa09d67804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotation\n",
    "import pandas as pd\n",
    "import pyranges as pr\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load the PCHiC data with specific data types defined or set low_memory=False\n",
    "pchic_data = pd.read_csv('PCHiC_peak_matrix_hg38_final.tsv', sep='\\t', dtype={\n",
    "    'baitChr': str, 'oeChr': str, 'baitID': str, 'oeID': str\n",
    "}, low_memory=False)\n",
    "\n",
    "# Rename columns to fit PyRanges requirements\n",
    "pchic_data.rename(columns={\n",
    "    'baitChr': 'Chromosome',\n",
    "    'baitStart': 'Start',\n",
    "    'baitEnd': 'End'\n",
    "}, inplace=True)\n",
    "\n",
    "# Filter out alt chromosome which is not in the main hg38\n",
    "pchic_data = pchic_data[~pchic_data['Chromosome'].str.contains(\n",
    "    '_alt', na=False)]\n",
    "\n",
    "# Create PyRanges object from the entire DataFrame\n",
    "pchic_ranges = pr.PyRanges(pchic_data)\n",
    "\n",
    "\n",
    "# Load the annotation data and adjust column names accordingly\n",
    "annotation_data = pd.read_csv('hg38_ensembl.txt', sep='\\t', dtype={\n",
    "                              'Chromosome/scaffold name': str, 'Strand': str})\n",
    "\n",
    "annotation_data['Strand'] = annotation_data['Strand'].replace(\n",
    "    {'1': '+', '-1': '-'})\n",
    "\n",
    "annotation_data.rename(columns={\n",
    "    'Chromosome/scaffold name': 'Chromosome',\n",
    "    'Gene start (bp)': 'Start',\n",
    "    'Gene end (bp)': 'End'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Create PyRanges object for annotation data\n",
    "annotation_ranges = pr.PyRanges(annotation_data)\n",
    "\n",
    "# Perform the join on overlapping coordinates\n",
    "merged_data = pchic_ranges.join(annotation_ranges, apply_strand_suffix=False)\n",
    "\n",
    "# Convert back to DataFrame for further manipulation or saving\n",
    "merged_df = merged_data.df\n",
    "\n",
    "# Rename and reorder columns to match your specific output requirements\n",
    "merged_df.rename(columns={\n",
    "    'Gene stable ID': 'ensg', 'Gene name': 'name', 'Gene type': 'biotype', 'Strand': 'strand', 'Chromosome': 'baitChr',\n",
    "    'Start': 'baitStart', 'End': 'baitEnd', 'Mon': 'Monocytes', 'Mac0': 'Macrophages_M0', 'Mac1': 'Macrophages_M1',\n",
    "    'Mac2': 'Macrophages_M2', 'Neu': 'Neutrophils', 'MK': 'Megakaryocytes', 'EP': 'Endothelial_precursors', 'Ery': 'Erythroblasts',\n",
    "    'FoeT': 'Foetal_thymus', 'nCD4': 'Naive_CD4', 'tCD4': 'Total_CD4_MF', 'aCD4': 'Total_CD4_Activated', 'naCD4': 'Total_CD4_NonActivated',\n",
    "    'nCD8': 'Naive_CD8', 'tCD8': 'Total_CD8', 'nB': 'Naive_B', 'tB': 'Total_B'\n",
    "}, inplace=True)\n",
    "\n",
    "# Example to reorder columns for final output, adjust according to your actual data structure\n",
    "final_columns = [\n",
    "    'ensg', 'name', 'biotype', 'strand', 'baitChr', 'baitStart', 'baitEnd',\n",
    "    'baitID', 'baitName', 'oeChr', 'oeStart', 'oeEnd', 'oeID', 'oeName', 'dist',\n",
    "    'Monocytes', 'Macrophages_M0', 'Macrophages_M1', 'Macrophages_M2', 'Neutrophils',\n",
    "    'Megakaryocytes', 'Endothelial_precursors', 'Erythroblasts', 'Foetal_thymus',\n",
    "    'Naive_CD4', 'Total_CD4_MF', 'Total_CD4_Activated', 'Total_CD4_NonActivated',\n",
    "    'Naive_CD8', 'Total_CD8', 'Naive_B', 'Total_B'\n",
    "]\n",
    "final_df = merged_df[final_columns]\n",
    "\n",
    "# Save to CSV if needed\n",
    "final_df.to_csv('annotated_PCHiC_peak_matrix_hg38_final.tsv',\n",
    "                sep='\\t', index=False)\n",
    "print('Annotation successfully added and saved as: annotated_PCHiC_peak_matrix_hg38_final.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b5a88-54b6-4e02-aa2b-df814d1846d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process pcHiC digest file\n",
    "from Bio import SeqIO\n",
    "from Bio.Restriction import RestrictionBatch, Restriction\n",
    "import pandas as pd\n",
    "\n",
    "# Load restriction HindIII\n",
    "rb = RestrictionBatch([Restriction.HindIII])\n",
    "\n",
    "digest_data = {\n",
    "    \"chr\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "# Parse by hg38 reference genome\n",
    "for genome in SeqIO.parse(\"hg38.fa\", \"fasta\"):\n",
    "    fragments = rb.search(genome.seq)\n",
    "    for start, end in zip([1] + [x+1 for x in fragments[Restriction.HindIII]], fragments[Restriction.HindIII] + [len(genome.seq)]):\n",
    "        digest_data[\"chr\"].append(genome.id.replace(\n",
    "            'chr', ''))  # Remove 'chr' prefix\n",
    "        digest_data[\"start\"].append(start)\n",
    "        digest_data[\"end\"].append(end - 1)  # Decrement end position by 1\n",
    "\n",
    "digest_df = pd.DataFrame(digest_data)\n",
    "\n",
    "# Filter out unwanted chromosomes\n",
    "valid_chromosomes = [str(i) for i in range(1, 23)] + ['X', 'Y']\n",
    "digest_df = digest_df[digest_df['chr'].isin(valid_chromosomes)]\n",
    "\n",
    "# Assign 'fragid' after filtering\n",
    "digest_df['fragid'] = range(1, len(digest_df) + 1)\n",
    "\n",
    "# Export to a BED file\n",
    "digest_df[['chr', 'start', 'end', 'fragid']].to_csv(\n",
    "    \"hindIII_digest_hg38.bed\", index=False, sep='\\t', header=True)\n",
    "print('pcHiC digest file successfully generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7e42d-61a7-4b95-b7e7-1b744be38190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process pcHiC design/annotation file\n",
    "import pyranges as pr\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load your BED files into PyRanges objects\n",
    "fragments = pr.read_bed(\"hindIII_digest_hg38.bed\",as_df=False)\n",
    "genes = pr.read_bed(\"hg38_ensembl.bed\",as_df=False)\n",
    "\n",
    "\n",
    "# Rename columns by accessing the underlying DataFrame\n",
    "fragments_df = fragments.df\n",
    "genes_df = genes.df\n",
    "\n",
    "fragments_df.rename(columns={'Name': 'fragid'}, inplace=True)\n",
    "genes_df.rename(columns={'Name': 'ensg'}, inplace=True)\n",
    "\n",
    "# Recreate PyRanges objects with the updated DataFrames\n",
    "fragments = pr.PyRanges(fragments_df)\n",
    "genes = pr.PyRanges(genes_df)\n",
    "\n",
    "chromosomes_to_keep = [str(i) for i in range(1, 23)]\n",
    "chromosomes_to_keep.extend(['X', 'Y'])\n",
    "filtered_fragments = fragments[fragments.Chromosome.isin(chromosomes_to_keep)]\n",
    "\n",
    "# Use PyRanges to find overlaps using 'any' as the join type\n",
    "overlaps = filtered_fragments.join(genes)\n",
    "\n",
    "# Print or save the results\n",
    "result_df = overlaps.df\n",
    "selected_columns_df = result_df[['fragid', 'ensg']]\n",
    "print(selected_columns_df.head())\n",
    "\n",
    "selected_columns_df.to_csv(\n",
    "    'design_PCHiC_hg38.tsv', sep='\\t', index=False)\n",
    "\n",
    "filtered_fragments_df = filtered_fragments.df\n",
    "filtered_fragments_df.rename(\n",
    "    columns={'Chromosome': 'chr', 'Start': 'start', 'End': 'end'}, inplace=True)\n",
    "\n",
    "print(filtered_fragments_df)\n",
    "filtered_fragments_df.to_csv('filtered_hindIII_digest_hg38.tsv', sep='\\t', index=False)\n",
    "\n",
    "print('design_PCHiC_hg38.tsv file successfully generated')\n",
    "print('filtered_hindIII_digest_hg38.tsv file successfully generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b98c3d-bcaf-4ced-9415-64282b653562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process cSNPs file (Recommend to use HPC)\n",
    "!sbash process_cSNPs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a21db9-9045-4248-9b22-e84569c69820",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035c40a-c344-40d1-a5cb-609aacaf5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bae98-a628-49ae-907a-804313e88d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install_github(\"ollyburren/rCOGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd308f2b-64c3-44f7-981e-ba7cc28c1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(GenomicRanges)\n",
    "library(devtools)\n",
    "library(rCOGS)\n",
    "library(magrittr)\n",
    "library(knitr)\n",
    "\n",
    "# Loaing Recombination Data\n",
    "ld.gr <- load_ld_regions('hapmap38_recomb.bed')\n",
    "(ld.gr)\n",
    "\n",
    "maf.DT <- load_ref_maf('referenced_maf.tsv',min.maf=0.05)\n",
    "head(maf.DT)\n",
    "\n",
    "t1d.DT <- load_gwas('trait_association.tsv',maf.DT,ld.gr,n.cases=5202,n.controls=1399360)\n",
    "head(t1d.DT)\n",
    "\n",
    "\n",
    "feature.sets <- make_pchic('annotated_PCHiC_peak_matrix_hg38_final.tsv',biotype.filter='protein_coding')\n",
    "names(feature.sets)\n",
    "\n",
    "\n",
    "# get a list of all genes included\n",
    "all.genes <- lapply(feature.sets,function(g) unique(g$ensg)) %>% do.call('c',.) %>% unique\n",
    "feature.sets[['VProm']] <- make_vprom('hindIII_digest_hg38.tsv','design_PCHiC_hg38.tsv',all.genes)\n",
    "\n",
    "csnps.gr <- make_csnps('cSNPs_file.tsv')\n",
    "head(csnps.gr)\n",
    "\n",
    "digest.gr <- load_digest('hindIII_digest_hg38.tsv')\n",
    "head(digest.gr)\n",
    "\n",
    "\n",
    "# compute overall cogs score\n",
    "suppressWarnings({\n",
    "overall.scores <- compute_cogs(t1d.DT,csnps.gr,digest.gr,feature.sets)\n",
    "})\n",
    "head(overall.scores[order(cogs,decreasing = TRUE),]) %>% kable\n",
    "\n",
    "# compute a T-cell specific score\n",
    "# target_tissue<-c('Total_CD4_Activated','Total_CD4_NonActivated','Naive_CD4','Total_CD4_MF','Naive_CD8','Total_CD8')\n",
    "# suppressWarnings({\n",
    "# tcell.scores <- compute_cogs(t1d.DT,csnps.gr,digest.gr,feature.sets,target_tissue) %>% head()\n",
    "# })\n",
    "# head(tcell.scores[order(cogs,decreasing = TRUE),]) %>% kable\n",
    "\n",
    "# sort result by cogs score\n",
    "overall.scores$cogs <- as.numeric(as.character(overall.scores$cogs))\n",
    "sorted_scores <- overall.scores[order(-overall.scores$cogs), ]\n",
    "\n",
    "write.table(sorted_scores, \"cogs_results.tsv\", sep=\"\\t\", quote=FALSE, row.names=FALSE)\n",
    "message(\"cogs_results.tsv successfully saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
